---
title: 扩散模型的动力学机制
date: 2024-05-09
math: true
---

关于一篇论文：Dynamical regimes of diffusion models (Biroli et al., 2024) 的笔记，感觉是一个很新颖的研究，使用统计物理的“它山之石”理解和解释深度学习，或许可以打开深度学习的“黑盒子”，更好设计和应用模型。

## 1 为什么需要“它山之玉”

目前关于扩散模型的理论研究主要聚焦于：

- 收敛性分析：研究在有限数据和有限时间下，扩散模型是否能够收敛到目标分布。
- 泛化能力：扩散模型如何在高维空间中生成合理的样本，而不会过拟合训练数据。
- 训练效率：优化方法如何影响扩散过程的稳定性和效果。

已有研究表明：

- 在有限维度数据情况下，扩散模型可以收敛到真实数据分布【相关文献：Benton et al., 2023】。
- 但在高维空间中，数据点之间的插值变得极其困难【Donoho et al., 2000】。
- 目前仍缺乏系统性的方法来解释扩散模型如何在高维空间保持生成质量。

仍然存在许多关键问题未解，包括在Sora发布后经过激烈讨论的，扩散模型生成视频是由于其学习的训练数据还是模型整整可以理解真实“物理世界”，如何真正学会真实“物理规律”，防止出现视频生成常见的“脚和足球擦肩而过或穿模”等常见错误：

1. 扩散模型的动态行为尚不清楚：
   - 在反向去噪过程中，数据如何逐步形成最终的结构？
   - 在不同的时间阶段，生成轨迹的特征如何变化？
2. 是否存在不同的生成阶段？：
   - 扩散模型在从噪声到数据的过程中，是否存在明显的相变或不同的动态机制？
   - 如何从理论上解释扩散模型的生成过程？
3. 记忆效应与泛化问题：
   - 扩散模型是否只是简单地记住了训练数据，而没有真正学会数据分布？
   - 训练数据的规模如何影响模型的泛化能力？
   - 维度大小与数据数量之间的关系如何影响模型的性能？

## 2 扩散模型基础

### 2.1 正向扩散过程（Forward Diffusion Process）

扩散模型的核心思想是用一个马尔可夫链（Markov Chain）逐步将数据转换为噪声，使得最终数据分布接近于标准高斯分布。给定一个真实数据分布 \( q(x_0) \)，正向扩散过程逐步向数据添加噪声，使得数据逐渐变为各向同性的高斯噪声。设 \( x_t \) 是在时间 \( t \) 时刻的随机变量，则正向扩散过程定义如下：

\[
q(x_t | x_0) = \mathcal{N}(x_t; \alpha_t x_0, \sigma_t^2 I)
\]

其中\( \alpha_t = e^{-\frac{1}{2} \int_0^t \beta_s ds} \) 控制数据随时间的衰减程度，\( \sigma_t^2 = 1 - \alpha_t^2 \) 控制噪声的增长，\( \beta_t \) 为正向扩散速率，\( \mathcal{N}(\mu, \Sigma) \) 表示均值为 \( \mu \)、协方差矩阵为 \( \Sigma \) 的高斯分布。等价地，正向扩散过程可以表示为一个伊藤过程（Itô Process），得到SDE之后变为熟悉的形式，可以使用更多关于随机过程或是统计物理的方法进行研究：

\[
dx = -\frac{1}{2} \beta_t x dt + \sqrt{\beta_t} dW_t
\]

其中：第一项 \( -\frac{1}{2} \beta_t x dt \) 控制数据向零衰减，第二项 \( \sqrt{\beta_t} dW_t \) 控制数据向高斯噪声演化，\( W_t \) 是维纳过程（Wiener Process）。当 \( t \to T \) 时，所有数据点都接近于标准高斯分布 \( x_T \sim \mathcal{N}(0, I) \)，即数据的所有信息在正向扩散过程中被遗忘。

扩散模型的目标是在反向生成过程中恢复原始数据，而恢复数据的关键是学习数据的对数概率梯度（Score Function）：

\[
s_t(x) = \nabla_x \log q(x_t)
\]

该得分函数描述了数据分布在不同时间 \( t \) 下的概率梯度方向。直观来说，它告诉我们如何调整数据点，使其更接近真实数据分布。

在实践中，得分函数通常通过得分匹配（Score Matching）方法学习：

1. 直接得分匹配（DSM, Direct Score Matching）：最小化 \( \mathbb{E}_{q(x_t|x_0)} [ || s_\theta(x_t, t) - \nabla_x \log q(x_t | x_0) ||^2 ] \)。
2. 去噪得分匹配（Denoising Score Matching, DSM）：学习噪声数据的得分函数，而不是直接拟合真实分布的得分函数。
3. 基于 Stein’s 公式的得分匹配：使用 Stein Method 学习得分函数，无需计算真实数据分布。

最终训练出的得分函数 \( s_\theta(x_t, t) \) 将用于反向扩散过程中恢复数据。

### 2.2 反向生成过程（Reverse Generation Process）

在正向扩散过程中，我们从数据分布 \( q(x_0) \) 逐渐向高斯分布 \( q(x_T) \) 添加噪声。为了生成数据，我们需要执行时间反演（Time Reversal），即从高斯分布 \( q(x_T) \) 逐步恢复数据。根据上个世纪80年代左右的一篇文章，可以证明反向扩散过程满足以下时间反演的随机微分方程（SDE）：

\[
dx = \left[\frac{1}{2} \beta_t x + \beta_t \nabla_x \log q(x_t) \right] dt + \sqrt{\beta_t} dW_t
\]

由于真实数据分布 \( q(x_t) \) 不可知，我们使用学习到的得分函数 \( s_\theta(x_t, t) \approx \nabla_x \log q(x_t) \) 近似代替它。最终，反向扩散过程可写为：

\[
dx = \left[\frac{1}{2} \beta_t x + \beta_t s_\theta(x, t) \right] dt + \sqrt{\beta_t} dW_t
\]

直观解释：反向过程中的第一项 \( \frac{1}{2} \beta_t x dt \) 试图恢复数据的原始结构，第二项 \( \beta_t s_\theta(x, t) dt \) 由得分函数引导数据向正确的方向修正，第三项 \( \sqrt{\beta_t} dW_t \) 表示扩散过程中的随机扰动。在代码实现中，反向生成过程通常有两种实现方式：

1. 随机微分方程（SDE）方法
   - 使用上述的随机微分方程（SDE）进行数据生成。
   - 通过数值方法（如 Euler–Maruyama 方法）对 SDE 进行离散化求解。

2. 确定性概率流（ODE）方法
   - 研究发现扩散模型的 SDE 具有确定性的等效形式：
     \[
     dx = \left[\frac{1}{2} \beta_t x + \beta_t s_\theta(x, t) \right] dt
     \]
   - 这个方程称为概率流常微分方程（Probability Flow ODE）。
   - 通过数值积分（可以调用各种已经非常成熟的库和方法，如 Runge-Kutta 方法）可高效求解。

扩散模型的反向生成过程如下：

1. 采样初始噪声：从标准高斯分布 \( x_T \sim \mathcal{N}(0, I) \) 采样一个随机噪声样本。
2. 时间步进：
   - 按照 SDE 或 ODE 进行离散化反向求解，从 \( x_T \to x_0 \)。
   - 每一步调整数据，使其更接近真实数据分布。
3. 最终输出：当 \( t = 0 \) 时，数据回归到目标分布 \( q(x_0) \)，即生成新的数据样本。

## 3 扩散模型的三个阶段

在高维数据生成过程中，扩散模型的反向轨迹经历随机布朗运动（Regime I）、物种分化（Regime II）和记忆塌缩（Regime III）三个阶段。这些阶段的划分源于统计物理学中的相变理论，可以能帮助我们理解扩散模型的泛化能力及其对训练数据的记忆机制。

这部分是这篇论文的核心部分，感觉可以基于此做一些其他实证性的研究，可以在一些领域和物理机制相结合，如果有效可以在很大程度上帮助新模型的设计和理解。

### 3.1 随机布朗运动阶段（Regime I）

该阶段对应于反向扩散过程的最初时刻，数据点仍然接近于标准高斯分布。由于得分函数 \( s_\theta(x, t) \) 在该阶段的影响较弱，数据点的演化近似于纯随机布朗运动（Brownian Motion）。数据点在该阶段仍然是无结构的，尚未表现出训练数据的类别信息。

在该阶段，反向扩散过程可以简化为标准布朗运动：

\[
dx = \sqrt{2} dW_t
\]

其中 \( dW_t \) 是标准维纳过程（Wiener Process）。这意味着数据点仅在高维空间中random walk，尚未受到数据分布结构的约束。当数据点逐渐受到训练数据的影响，并开始向某些类别靠拢时，系统进入物种分化阶段（Regime II）。这个转变由数据协方差矩阵的主成分谱决定。

### 3.2 物种分化（Speciation）阶段（Regime II）

在该阶段，数据点的随机运动不再是无序的，而是开始偏向特定类别。这一现象类似于物理中的对称性破缺（Symmetry Breaking），即系统从无序状态演化出结构性模式。该阶段仍然具有一定的泛化能力，即生成样本不一定是训练数据本身，但开始表现出数据分布的主要类别特征。数据点的分化行为受数据协方差矩阵 \( C_0 \) 的主成分控制。对于高维数据，其主成分通常对应于数据的主要类别结构。数据的时间演化方程可以写为：

\[
dx = -x dt + \nabla_x \log q(x_t) dt + \sqrt{2} dW_t
\]

在此阶段，数据点的运动受到协方差矩阵的主成分 \( \Lambda \) 的影响，其分化时间 \( t_S \) 可由以下方程估计：

\[
\Lambda e^{-2t_S} = 1
\]

其中：\( \Lambda \) 是数据协方差矩阵的最大特征值。\( t_S \) 反映了数据从随机噪声到分类特征形成的时间。在高维极限下，通常有 \( \Lambda \propto d \)，因此 \( t_S \) 的增长遵循对数规律：

\[
t_S \approx \frac{1}{2} \log d
\]

说明，在更高维度的数据空间中，物种分化阶段持续的时间更长。

### 3.3 记忆塌缩（Collapse）阶段（Regime III）

在该阶段，数据点不再表现出泛化能力，而是逐渐收敛到训练数据点。这一现象类似于玻璃相凝聚（Glass Phase Condensation），即数据点的自由度减少，被吸引到训练数据的吸引子（Attractor）上。在极端情况下，扩散模型可能仅仅记住了训练数据，而不能生成新的样本（即过拟合）。数据点的塌缩时间 \( t_C \) 由信息熵变化决定。定义过剩熵（Excess Entropy）为：

\[
s(t) = -\frac{1}{d} \int q(x_t) \log q(x_t) dx
\]

在塌缩阶段，数据的分布变得高度集中，并形成多个孤立的高斯峰，这导致系统的熵降低。塌缩时间 \( t_C \) 由以下条件确定：

\[
s(t_C) = s_{\text{sep}}(t_C)
\]

其中，\( s_{\text{sep}}(t) \) 是由 \( n \) 个分离的高斯峰组成的分布的熵：

\[
s_{\text{sep}}(t) = \frac{\log n}{d} + \frac{1}{2} + \frac{1}{2} \log(2\pi \Delta_t)
\]

在大维度极限下，可以推导出塌缩时间的近似表达式：

\[
t_C = \frac{1}{2} \log \left( 1 + \frac{\sigma^2}{n^{2/d} - 1} \right)
\]

从该公式可以看出：如果 \( n \) 远小于 \( e^d \)，则 \( t_C \) 可能非常小，意味着数据很快塌缩到训练集。如果 \( n \) 远大于 \( e^d \)，则 \( t_C \) 可能很大，意味着模型仍然能够泛化，不会过拟合。这揭示了扩散模型的维度诅咒：为了避免记忆塌缩，训练数据的数量必须随着维度呈指数级增长。

### 3.4 计算 \( t_S \) 和 \( t_C \) 的方法

物种分化时间 \( t_S \) 由数据协方差矩阵的谱分析确定：

1. 计算训练数据的协方差矩阵 \( C_0 \)：
   \[
   C_0 = \mathbb{E}[(x - \mu)(x - \mu)^T]
   \]
2. 计算 \( C_0 \) 的最大特征值 \( \Lambda \)：
   \[
   C_0 v = \Lambda v
   \]
3. 代入公式：
   \[
   \Lambda e^{-2t_S} = 1
   \]
   求解 \( t_S \)。

记忆塌缩时间 \( t_C \) 由过剩熵分析确定：

1. 计算当前时间 \( t \) 的数据熵：
   \[
   s(t) = -\frac{1}{d} \int q(x_t) \log q(x_t) dx
   \]
2. 计算分离高斯分布的熵：
   \[
   s_{\text{sep}}(t) = \frac{\log n}{d} + \frac{1}{2} + \frac{1}{2} \log(2\pi \Delta_t)
   \]
3. 求解方程：
   \[
   s(t_C) = s_{\text{sep}}(t_C)
   \]

## 4 维度诅咒（Curse of Dimensionality）与扩散模型

### 4.1 维度和数据量对扩散模型的影响

维度诅咒是指在高维空间中，许多统计和机器学习方法的性能会急剧下降，主要原因包括：

- 数据稀疏性：在高维空间中，数据点之间的平均距离增加，使得局部结构变得不明显。
- 体积指数增长：在 \( d \) 维空间中，超立方体的体积随 \( d \) 指数级增长，导致数据点的分布变得极端稀疏。
- 计算复杂度提高：许多算法的计算复杂度依赖于维度 \( d \)，在高维情况下训练和推理的成本显著上升。

在扩散模型中，数据点在高维空间中的演化受到以下因素的影响：

- 正向扩散过程中，噪声主导数据点的分布：在低维空间中，噪声扰动较小，数据仍然保留一定的结构信息。在高维空间中，数据点在每个维度都受到噪声干扰，导致结构信息丢失更快。

- 反向生成过程中，得分函数的学习变得更困难：在高维空间中，得分函数的梯度方向可能变得不稳定，导致生成轨迹容易偏离真实数据分布。得分函数估计误差可能随着维度的增加而放大。

- 维度影响泛化能力和生成质量： 维度 \( d \) 越高，训练数据覆盖整个数据空间的难度越大，导致模型更容易记忆训练数据（即记忆塌缩）。生成的样本可能偏向于训练数据集，而不是从真实数据分布中采样。

在高维空间中，数据的熵（信息量）增加，但训练数据的数量 \( n \) 通常不足以覆盖整个数据空间。为了避免记忆塌缩，数据点的数量必须指数级增长：

\[
t_C = \frac{1}{2} \log \left( 1 + \frac{\sigma^2}{n^{2/d} - 1} \right)
\]

该公式表明：当 \( n \gg e^d \) 时，塌缩时间 \( t_C \) 增加，模型具有更好的泛化能力。当 \( n \ll e^d \) 时，模型很快塌缩到训练数据，导致生成数据与训练数据几乎相同（即过拟合）。

### 4.2 训练数据数量对避免记忆塌缩的作用
### (1) 记忆塌缩的机制
记忆塌缩（Collapse）是指在扩散模型的反向生成过程中，生成样本最终收敛到训练数据点，而不是从真实数据分布中采样。这种现象发生的原因包括：
- 数据不足：当 \( n \) 过小（相比于 \( d \)），扩散模型难以学习完整的数据分布，而只能记住训练数据。
- 得分函数的过拟合：在数据不足的情况下，得分函数 \( s_\theta(x, t) \) 可能学会了将数据点直接推回到训练集，而不是沿真实数据分布扩展。
- 熵降低：当 \( n \) 过小时，数据的经验熵（Empirical Entropy） \( s_{\text{sep}}(t) \) 远小于理论熵 \( s(t) \)，导致塌缩。

### (2) 增加训练数据数量的作用
为了避免记忆塌缩，需要确保训练数据的覆盖度足够，通常的策略包括：
1. 增加数据规模 \( n \)：
   - 使得数据点的分布更接近真实分布，降低塌缩风险。
   - 避免得分函数学习到过于局部化的信息。
  
2. 使用数据增强（Data Augmentation）：
   - 通过旋转、缩放、裁剪等方法增加数据的多样性，提高模型的泛化能力。

3. 正则化得分函数：
   - 采用 Smooth Score Matching 等方法，使得得分函数对噪声更加鲁棒，而不是直接记住训练数据。

4. 优化训练策略：
   - 采用更高效的优化方法（如 AdamW），减少梯度爆炸，提高高维空间中的学习稳定性。

实验表明：
- 在 CIFAR-10、ImageNet 等数据集上，使用较大的数据规模 \( n \) 可以有效推迟塌缩时间 \( t_C \)，提高生成质量。
- 但如果 \( d \) 过高，即使数据量增加，也可能难以完全避免塌缩。

---

## 4.3 近似得分函数的实际应用
### (1) 得分函数的近似误差
在实际应用中，我们不可能精确获得数据的真实得分函数 \( s(x, t) = \nabla_x \log p_t(x) \)，因此需要用神经网络 \( s_\theta(x, t) \) 进行近似。然而：
- 高维情况下，得分函数的估计误差可能会放大，导致生成轨迹不稳定。
- 噪声影响较大，如果训练数据不足，得分函数可能会学到错误的梯度方向。

### (2) 解决方案
为了缓解得分函数的误差，可以采用以下方法：
1. 正则化得分函数：
   - 通过对抗训练（Adversarial Training）或平滑方法（如 Stein Score Matching）提高得分函数的稳定性。
  
2. 改进神经网络架构：
   - 使用更强的 Transformer 或 Diffusion U-Net 结构，提高得分函数的拟合能力。

3. 调整训练数据分布：
   - 通过重要性采样（Importance Sampling）或流形学习（Manifold Learning）优化得分函数估计。

### (3) 实验结果
- 在 ImageNet 数据上，使用正则化得分函数可以减少模式崩溃，提高生成样本的多样性。
- 在低维数据（如 MNIST）上，得分函数的近似误差对生成影响较小，但在高维数据（如 LSUN）上，误差显著增加。

---

## 5. 总结
- 高维空间导致数据稀疏，使得扩散模型难以泛化（维度诅咒）。
- 需要指数级增长的数据量来避免记忆塌缩，否则模型只能记住训练数据。
- 优化得分函数（如正则化、数据增强）可以改善生成质量，减少塌缩风险。

通过优化数据规模、得分函数估计和训练策略，可以提升扩散模型在高维空间中的泛化能力，生成更真实的样本。


### 主要研究问题
1. 扩散模型的动态阶段划分
   - 研究扩散模型的生成过程中是否存在明确的动态阶段，并量化这些阶段的边界。
   - 通过解析方法和数值实验，分析数据如何从噪声状态逐渐生成清晰样本。

2. 物种分化（Speciation）现象
   - 研究在反向去噪过程中，数据是否会自发聚类，形成不同类别（类似物理中的对称性破缺）。
   - 通过数学分析推导出物种分化时间 \( t_S \)，即从噪声到可识别类别的转换时间。

3. 记忆塌缩（Collapse）现象
   - 研究在后期去噪过程中，数据点是否会“塌缩”到训练数据本身，而不是生成新的数据。
   - 研究塌缩时间 \( t_C \)，即模型从泛化能力较强的状态进入纯记忆状态的时间。

4. 高维空间中的维度诅咒
   - 研究在大数据和高维度的情况下，扩散模型如何受限于维度诅咒。
   - 研究数据点数量 \( n \) 和数据维度 \( d \) 之间的关系对模型泛化能力的影响。

### 研究方法
为了研究上述问题，本文采用以下方法：
- 统计物理分析：
  - 使用相变理论（Phase Transition Theory）分析生成过程中的关键转变点。
  - 研究高维高斯混合模型，提供解析解来刻画生成过程的不同阶段。
- 数值实验：
  - 在标准数据集（如 CIFAR-10、ImageNet）上验证理论预测。
  - 通过数值模拟研究不同数据规模对模型性能的影响。

### 研究贡献
本研究的主要贡献包括：
1. 提出扩散模型的三阶段理论框架：
   - 识别出布朗运动阶段（I）、物种分化阶段（II）、记忆塌缩阶段（III）。
   - 提出计算物种分化时间 \( t_S \) 和塌缩时间 \( t_C \) 的数学方法。
2. 揭示高维数据的生成机制：
   - 解析扩散模型如何在高维空间避免维度诅咒，并推导泛化能力的数学条件。
3. 提供理论指导，优化扩散模型的训练：
   - 通过分析模型的动态机制，提出避免记忆塌缩的方法，如正则化技术和训练数据规模调整策略。



## 4. 维度诅咒与扩散模型
   - 维度和数据量对扩散模型的影响
   - 训练数据数量对避免记忆塌缩的作用
   - 近似得分函数的实际应用

## 5. 解析解与数值验证
   - 5.1 高维高斯混合模型的解析解
   - 5.2 真实数据（如CIFAR-10、ImageNet）上的实验验证
   - 5.3 结果与理论预测的符合性

## 6. 物理视角下的相变分析
   - 物种分化的对称性破缺解释
   - 记忆塌缩的玻璃态相变解释
   - 统计物理方法在扩散模型分析中的应用

## 7. 未来研究方向
   - 在非精确得分函数学习条件下的研究
   - 结合正则化方法提高扩散模型的泛化能力
   - 更复杂数据集的应用分析

---

这个大纲涵盖了论文的主要理论框架，并组织成清晰的知识结构，适合用于深入研究扩散模型的动态机制。你可以根据具体需求对其进行调整和扩充。